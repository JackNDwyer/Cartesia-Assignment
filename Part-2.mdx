## Part 2: Dashboarding
MKT = Marketing, ACT = Activation, RET = Retention, GEN = General

- **MKT, Weekly:** Page visits/Website Actions -> Sign Up – gives insight into website touchpoints that resonate most. Pages, demos, etc.
- **ACT, Weekly:** Pageview -> Sign Ups -> Activations (generated 120 sec TTS snippets in dashboard or made API call) – general pulse on the top of funnel. Ideally, this steadily grows over time and most sign-ups activate.
- **ACT, Weekly:** API activations (made successful API call) – are users successfully running code with Cartesia's API? This signals strong intent.
- **ACT, Weekly:** Github connections – a leading indicator that devs plan to build agents beyond the initial text-to-agent.
- **ACT, Weekly:** Line Agent activations (2+ realtime phone-based calls on an agent) – making agents is straightforward, but multiple phone calls with an agent shows intent to use in production.
- **RET, Monthly:** WAU split (New, Returning <7 days since last generation, Resurrecting >7 days since last, Churned)
- **RET, Monthly:** 7 Day Retention – do users stick with the product 1–3 weeks later? This is probably the most important metric for sustained growth and satisfaction.
- **RET, Monthly:** Account level growth – are accounts increasing usage week over week?
- **GEN, Monthly:** WAU for each playground feature – what do people gravitate toward? What’s unused? Likewise for API calls, voices, emotes, etc. This gives an overview of overall product footprint.
- **GEN, Monthly:** Minutes of generated Speech, Transcription, Realtime – general product usage over time. More usage = more value delivered.
- **GEN, Weekly:** % of TTS Regenerations - what percentage of TTS generations get re-done with similar parameters. Fewer regenerations = better first-time generations.
- **GEN, Weekly:** TTS Attributes - which languages, voices, and emotions are users gravitating toward? Will help us identify which TTS attributes to invest more into.

### Metrics Review Cadence & Audience

Metrics are reviewed **weekly** in cross-functional product/leadership sync and **monthly** in all-hands or business review meetings. Key product-level metrics surface in dashboards visible to all teams. Feature-level or experiment-specific metrics are highlighted in retrospectives or post-launch summaries.

Post-launch, these metrics directly inform prioritization, roadmap, and iteration focus (e.g., outages, shifting sentiments, features with stalled adoption trigger reviews, outperforming features get upgrades and suggest new markets for GTM).

Evangelizing the culture comes in three ways:

1. Practice what I preach. When discussing product changes or outcomes, include a relevant metric whenever possible.
2. Every product change should mostly impact 1–2 core team metrics. Everything should help users build better AI voice agents, and the impact should be visible in those metrics. You can even gamify it so the team sees the value of their work directly.
3. Bring metrics to the team. If there are TVs in office, display the main metrics. During meetings, tie asks and outcomes to metric deltas.

For example, for the emotion tagging feature, the product manager and engineers should know why it’s being built (use cases that require expressive voices + increase adoption), and once released, where to view adoption and retention metrics. After a week, add a short slide to all-hands showing before/after and adoption numbers to tie real business value to the change.
