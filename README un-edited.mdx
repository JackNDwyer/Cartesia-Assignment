**I am using AI to proofread and clean up my code, but I wrote 100% of this myself as a first draft.**

First Impressions (90 min)

Being that I am familiar with Cartesia, I will be assuming the persona of a developer with a medium level of familiarity with Voice Agents. I am in search of an end-to-end solution to create a voice Agents to handle support calls for a health tech startup in El Paso, Texas that handles appointment scheduling for English and Spanish speakers. I have a few workflows I'm hoping to build, each needing decision trees and tool calling. I have played with LiveKit, Vapi and ElevenLabs, and also tried OpenAI's voice stack (TTS, STT, PipeLine vs Speech-to-speech, Realtime). 

I saw the Sonic 3 release on X and noticed Cartesia also has a voice agents product, so I am visiting the website for the first time to learn about Sonic 3, but also their voice agents product.

# Part 0: Website/Dashboard First Impressions (90 min including typing)

## Website Homepage
<hero-top-landing>

At first glance, I'm seeing that Cartesia is primarily a text-to-speech inference provider. The homepage directs me to /sonic. The entire upper 1/2 of the homepage is centered on the new Sonic TTS model: it's fast, natural, and expressive, with voices for a variety of use cases.

### Idea: As the Line and Ink product mature, create a new homepage that touches on all of these models (assuming strategically Cartesia wants to push on those).

<agents-powering-sonic-page>

As I get down to the "Powering Agents" section, I initially believed it would be talking about building voice agents, but instead it's continuing to talk about how Cartesia's TTS is powering voice agents.

I do like that multi-lingual is mentioned, with different dialects of Spanish being listed.

I see there is a TTS API, as well as an SDK. I also see there is SOC 2, HIPAA, PCI compliance.

There is no mention of agents (and STT for that matter) on the homepage.

### Idea: Mention Line Agents and Ink STT with links so users can cmd-click to open those pages in the background.

## Website Agents Page
With a grasp of the TTS product, I see "Agents" in the header.

<hero-line>

I'm now looking at what I'm hoping to integrate. I see the text "Line is a code-first ecosystem to get from zero—to your first agent—to your best agent, in record time."

It's unclear what "Line" is: both the word "Line" and how it's used, as well as a "code-first ecosystem". Seeing as I clicked Agents, I'm assuming line is an agent builder.

### Idea: Rework this slightly to something like "Line is a code-first agent builder to rapidly prototype and deploy flexible voice agents for any use case.

<line-start-anywhere>

This section is inspiring confidence that you have what I need. I already have 2 LLMs my company uses with (OpenAI and Gemini) which seems compatible with your system, and it looks like you have templates to help me get started, as well as a text-to-agent feature.

<agents-features>

I do not see any mention of tool calling (other than a hidden RAG-focused Tool Calling bullet in the "Build" section). I see no mention of building workflows/decision trees/handoffs either. It would also be nice if I could talk to one of the realtime agents to get a feel for quality/latency in production. The other websites let me do that rather than just show me snippets.

### Idea: Communicate tool calling and handoffs/workflows more clearly.
### Idea: Add a realtime demo.

As I continue scrolling down, it looks like there are a few industries listed, including my industry of Healthcare. I click on it.

<healthcare-page>

This page inspires confidence via social proof and real world examples. I still would like to try one of the voice agents—either via a widget or phone number—but it gives me confidence.

I click "Give it a try" and am taken to a Sign Up page. Before I sign up, I want to view pricing and the docs. There is no navigation on the sign up page, but before hitting the back button, I notice an small ? and on click, it's actually a menu. I click `Pricing` first.

<sign up page>

### Idea: Make the menu button more clear or add simple navigation to docs/pricing. The ? suggests that it's going to be support, but not necessarily docs/pricing.

## Website Pricing Page
<pricing-main>

This page generally makes sense. It's busy, but as I focus on what I need, it makes sense.

I see a bunch of numbers here: monthly fees, credits, and agent credits. I don't think I need cloning, but I do work at an organization that expects up to 5 calls at once across 4 different agents.

The startup plan seems okay for my use case. 5 concurrent on TTS, 5 agent slots with 20 concurrency.

### Idea: Startup Plan = 20 Agents concurrency != 5 TTS concurrency. I understand the AI isn't always talking, but seems like these numbers should be closer.

Pricing is somewhat difficult to calculate. There is duration based pricing for the Line Agents and STT, but usage-based for the TTS model.

I then go back to the Sign Up page, which has a strong emphasis on Sonic 3.

Idea: A testimonial or more use cases/functionality could be useful here. Right now it's another announcement about Sonic 3.

## Docs
The Docs are very strong. Going through sequentially via the bottom nav was nice.

Generating a snippet via cURL/Python/JS makes sense, but since I'm looking to create a voice agent, doing a bytes call, which seems more suited for batch calls, isn't super relevant to me.

I got it working very easily, and the token based auth makes sense.

```
curl -N -X POST "https://api.cartesia.ai/tts/bytes" \
        -H "Cartesia-Version: 2025-04-16" \
        -H "X-API-Key: $CARTESIA_API_KEY" \
        -H "Content-Type: application/json" \
        -d '{
            "transcript": "Howdy!",
            "model_id": "sonic-3",
            "voice":
                {
                    "mode":"id",
                    "id": "694f9389-aac1-45b6-b726-9d9369183238"
                },
            "output_format":
                {
                    "container":"mp3",
                    "bit_rate":64000,
                    "sample_rate":44100
                }
            }' > howdy_sample.mp3
```

I'm not yet sold on using Cartesia's Agents product yet, but I do like the TTS product and regardless of which agents platform I use, I'll likely use Sonic TTS. I want to find a Websocket endpoint. I eventually find it in the API reference.

<docs-formats>

### Idea: Link to the different TTS endpoints in the info box shown in the screenshot.

## Playground Text to Speech

**Note: I know your product very well so I'm going to continue writing from the persona of the dev**

<tts-panel-with-banner>

After signing up, the first thing I'm met with is a `Text to Speech page`. I see the banner on top showing me Sonic-3 is live. On my Macbook Air, this eats a significant amount of screen real estate.

### Idea: The banner can be shrunk to one line of text. It current spans more than 2.

I see a strong emphasis on Voice Tools (it's the top section) on the sidebar.

There are a few example prompts to get started with. Some appear to be realtime agents "Schedule an appointment" or "Concierge". When I click them, they fill the Text-to-Speech box with a sentence.

This is useful, as I'm learning the tool and getting to try different examples, but when I click one example prompt, they all disappear, and if I hit speak, the text remains. Additionally, when I did realize I needed to erase the text to get the example prompts back, they're different examples.

### Idea: Either have an example prompt library that can easily be scrolled through, or pick a set of standard example prompts. And let me select other example prompts without needing to delete the populated text.

<emote-sonic-2>

Looking at the "Controls", I see multiple models. It defaults to Sonic 3. I felt compelled to try different models, and when I swap models from Sonic 3 to Sonic 2 *after* clicking an example prompt, the emotion and emote tags remain, which was odd since they don't work with Sonic 2. The model spoke the emote "Laughter" for Sonic 2.

### Idea: When I change models, remove the emotion tags, or at least ignore have the TTS model ignore them if they aren't compatible and provide a warning "model not comptaible with emotion tags".

Otherwise controls makes sense. Speed and Volume are inuitive. Transcription Language I just assume lets me generate specific language voice snippets, even if the mismatches with the Voice language.

<emoji>

When I use Sonic 3, emotions (reflected by different emojis) don't reflect anywhere in the text box, but they do highlight on the sidebar, which I assume means they're working.

### Idea: The emoji buttons in the `Text to speech` page should add the text tag in the text box. Right now users can either add a text tag or use the emoji. If Cartesia wants to train user on the text tags, tags should always be shown in the text box.

I then clicked "Voice", which looks like a drop down but then a modal popped up.

It started on `All` and then when I click "Cartesia Voices" nothing changes.

<all-voices>
<cartesia-voices>

When I do clone a voice and use it, the `RECENTLY USED` section in the `All` panel and the `Cartesia` panel remain identical. I'd expect `All` to include the new cloned voice I just used. If I scroll down in `All`, I eventually get to my cloned voice, but I don't understad the default sort order.

### Idea: Order the `All` voices in a way that is understandable to the user, and ensure the `RECENTLY USED` section in the `All` panel reflects cloned voices that are used. On refresh it didn't update.

<my-voice-tts>

The "My Voices" panel is empty.

<my-voices-voices>

### Idea: Add a link to the cloning page in the empty state so I can create my own voice if I want to. Similar story for "Starred". Instead of an empty state, give me a CTA to `Star` my favorite voices for easy access. This is already in place for the empty state of `My Voices` on the `Voices` page.

I like that on the voices page I can play a voice snippet from the list, however to get the voice ID, I have to perform 2 clicks, with one of them opening a menu that gives 2 options: "Copy ID" and "Copy Link". The "Copy Link" url takes me to a page that doesn't seem to have anything I can't already see on the Text To Speech page.

### Idea: Remove the "Copy Link" option and simple have a button to "Copy ID" at the top level of every voice.

## Other Voice Tools

Clicking through the other voice tools quickly, the cloning options make sense. Localize a voice I don't fully understand, and there isn't anything explaining it on the page. Voice changer is similar, I can inuit what it means but it's unclear.

### Idea: Add some text to explain the localizing and voice changer pages.

## Library

I see there is a separate `Library` section. I feel like `Voices` could be a standlone section or combined with `Text to Speech`, and the `Pronunciation` is definitely a good candidate for `Voice Tools`.

### Idea: The sidebar can be consolidated into `Voice`, `Line`, and `Platform`. `Voice` can contain everything it currently does + `Pronunciation`. The `Voices` and `Text to Speech` pages can be consolidated (primary value to user is finding a voice they like and generating snippets), and `Narration`, unless it's highly utilized, seems like it's trying to eat some of ElevanLabs lunch but is ultimately redundant.

## Studio

I don't understand what makes the narration section different from the Text-to-Speech page. Both have exports and allow you to view past generations. I believe Narrations allows you to save the audio file as well, but otherwise not materially different and that would be easy to add to the `Text to speech` page.

## Line
<agent-ftux>

I clicked around this panel.

First I tried Text-to-agent and as far as I can tell all it did was use what I typed in. It's unclear if I can edit this agent in Github. I have to promote it to production before being given the option.

<text-to-agents-prompting>

### Idea:  Users will want to get a warm start with Text-to-agent. It doesn't provide much value at the moment, but with tool-calling and editing the code it could. Generally tool-calling feels like a big opportunity.

Then I tried "Connect your code".

I connected my github and it asked me to link a repo. I didn't have one, so I backed out of that menu.

### Idea: For the FTUX, maybe we pre-seed their account with a starter agent. The current text-to-agent doesn't provide a ton of value, so having them go through steps to make one is low ROI for the user. Another option is nudge them toward one of the templates. Potentially instead of the empty list of agents just saying "empty", there's an opportunity there for a CTA.

Then the templates available were of mixed value. The Basic Chat was fine, but I felt it was too basic. The Form Filler was very good and quite comprehensive.

### Idea: I didn't see any transfer logic or ability to update agent prompts.
### Idea: Link to the github repo from within the Cartesia dashboard.

The repo itself is clear. Good README, the code is clean. My one gripe is maybe just how much code there is. It's very low level, which limits my ability to quickly test. I can use Cursor to make edits, which solves most of that problem.

Part 1: (20 min)

It was very easy to get started. The homepage is very TTS-heavy given it's the Sonic page. There wasn't anything on the page indicating that STT or Agents are available, so I had to go into the top navigation bar.

I also would have liked to try a realtime demo on the Agents page. Voice agents are a significant part of the business, and latency is one of our differntiators. Let devs experience Line/the latency first hand so they feel comfortable rolling it out to their customers.

Pricing was initially overwhelming, but once I got a grasp of it for a specific use case, it makes sense. The mismatch between concurrency for TTS vs Agents was the only thing that seemed amiss. I understand why TTS concurrency is lower (capacity planning for GPUs), but is worth noting that from a dev-perspective I can't utilize additional agents without TTS.

The `Getting Started`/`Models` docs are very good. My only complaint is there was only a passing mention of the other available TTS endpoints and Bytes data types. For developers who want to use the Websocket endpoint, which I imagine is a large percentage, there should be some mention in the `Getting Started` or `SDKs` sections.

*Disclaimer: My Python is rusty.* The `Agents` docs are very comprehensive. The amount of first class citizens is overwhelming and the things I cared about (e.g. tool-calling) were more complicated than I'd hoped. I imagine they are optimized for AI to comprehend, but as a developer, I still would like the docs to be more approachable - even a simple diagram would have made a big difference.

I didn't see a way to bring my own phone number from Twilio or update prompts/transfer to a human. If Line is going to be a low-level of abstraction, those two things are an opportunity. I believe I could figure those out (maybe) with custom Python.

The playground was very easy to use. I was able to generate a snippet quickly and try a handful of voices with specific vernacular to my use case. I ran into some confusion when I clicked on the example prompts. There was not a simple way to try different example prompts without deleting the populated example prompt, and if I select an example prompt and then change the model from Sonic 3 to Sonic 2, the TTS model will speak out emotes like "Laughter". Also, even after using a cloned voice, my "Cartesia Voices" vs "All Voices" tabs were identical and I didn't understand the sorting of voices.

I liked the options for the pronunciation and cloning, however it is all a bit scattered. I count 8 different pages in the playground for voice, yet I imagine this can be condensed down to 3-4.
1. `Narration` is very similar to `Text to Speech`. Adding an option to save generations would make them functionally identical.
2. `Voices` is identical to the *Voices* menu in `Text to Speech`.
3. The two different cloning pages can be consolidated.
4. `Localization` and `Voice Changer` can be consolidated into an `Voice Studio` page. 

The Agents page is where I ran into some headaches. `Text-to-agent` is underwhelming. I saw no option to add tool calling and the single prompt agent wasn't very useful to any business use cases I can think of (at least without tool calling)

Additionally, to really get started with Agents is a large investment. I initially saw an option to `Connect Your Code` which asked for a Github repo. I didn't have one, so instead I had to connect my Github to copy a template.

There was a lot of friction there.

Metrics I only breifly looked at. It seems okay for an MVP. So long as users can get an aggregate view of call outcomes, it's in a good spot.

The three things I would change:
1. The homepage is the first touch point to frame our offering and build trust. If Cartesia is positioning itself as a one-stop Voice Agent platform (which I was told it is), we should say more about Line/Ink and frame it as a comprehensive offering. TTS is over-represented seeing as the landing page is /sonic (demo, latency, compliance, flexibility). If we are going to add Line, we should have a demo so devs can try a voice agent. Currently devs must go through the multiple steps to make one themselves before trying one.
2. Consolidating the sidebar would be a good way to clean up the playground. As mentioned above, there is redundancy across Text-to-speech, Voices, and Narration. There is also a voice-specific page that conveys the voice name, ID, language that. I don't believe this provides any utility and if anything creates more cognitive overhead for users we ultimately want generating snippets and creating agents.
3. Text-to-agents should allow for adding tool calls. Voice agents exist to replace humans that talk + do things. As I understand it, the Text-to-agents feature generates Voice Agents that can talk but not do things. This is a bigger lift given agents/tool calls are represented as code, but until then I suspect devs are finding this feauture underwhelming.


Part 2: (30 min)
- MKT, Weekly: Website visits, broken down by UTM parameter. Checked weekly. Useful metric for monitoring general traffic to the website, and to gauge which channels are working for bringing people into the funnel.
- MKT, Weekly: Page Visits/Website Actions -> Sign Up - provides a view into which touch points on the website are resonating with visitors. Pages, demos, etc.
- ACT, Weekly: Pageview -> Sign Ups -> Activations (generated 2-3 TTS snippets/made API call/created agent) - general pulse on intentful top of funnel. Ideally it is steadily growing over time and majority of sign ups activate.
- ACT, Weekly: API Key creations - this is a great leading indicator for the kind of usage we're looking for.
- ACT, Weekly: API activations (2+ successful calls to the API) - are users successfully running code that incorporates Cartesia's API products? This is very intentful.
- ACT, Weekly: Github connections - This is a leading indicator that devs intend on making agents outside of the initial text-to-agent.
- ACT, Weekly: Line Agent activations (2+ realtime performed on the agent) - making the agents is relatively straightforward, but having multiple calls on an agent shows that it was created with intent to be used.
- RET, Monthly: WAU split by New, Returning (< 7 days since last generation), Ressurecting (> 7 days since last generation), Churned
- RET, Monthly: 7 Day Retention - Are activated users sticking with the product 1, 2, 3 weeks later? Probably the most important metric for sustained growth and customer satisfaction.
- RET, Monthly: Account level growth - are accounts using the product more or less as the weeks go on.
- GEN, Monthly: WAU for each feature in the playground - Which features are people gravtitating toward? Which are unused? I would do a similar thing for API calls, voices, emotes, etc. It's very broad, but gives a good overview of the entire product footprint.
- GEN, Monthly: Minutes of generated Speech, Transcription, Realtime - Metric to track general product usage over time. More usage = more value delivered

Evalgalizing the culture comes in two ways:
1. I'd need to practice what I preach. When I talk about product changes or outcomes, when possible and relevant, there should be a metric associated with it.
2. Every product change should more or less have an impact on 1-2 core metrics that the team is aware of. Everything built should be to help users build better AI voice agents, and that connection should be represented in those metrics. You can even gamify it a little bit so the team can see the impact of their work directly.
3. Bring the metrics to the team. If there are TVs, display the main metrics. During meetings, tie asks to metrics and outcomes to deltas.

An example, for the emotion tagging feature, the product manager and the engineer(s) building the feature should know why it's being built (to allow devs who need emotive voice to access it, which should increase adoption and expand usage to more use cases), and once it goes out, where to look to view adoption and retention metrics. After a week, add a short slide to all-hands showing before and after + adoption numbers to show cause and effect and tie real business value to the change.

# Part 3: (30 min)
What would you build?
What metric would it impact?
How much of a lift can you expect?
If necessary, feel free to call out your assumption (intuition-based) on what is the baseline for that metric.

 My experiments would be:
## 1: Create an email drip campaign for unegaged users (from the suggested templates)

Many users sign up and churn at various stages for various reasons. We have information on when the user signed up and what they've done in the UI, and there are often clues in the usage data as to why they churned. I've successfully run these campaigns in the past where certain emails are fired based on a combination of user product activity and timing.

For example, if 48 hrs have passed since the user signed up and they haven't generated anything, send them a series of emails showcasing the product, explainaing the free credits, and other things that would convince a user to initially try the product.

The primary metrics impacted are resurrections (get people back in when new features drop) and activations (convince people who have signed up to try the product).

Lift depends on what the user last did. If they had already been an active, paying customer and then left, they may have gone out of business or found another provider

My guess is something like 40% of sign ups activate, and only 20% of those (so 8% of total sign ups) go on to become paying users. With the campaigns, we will likely lift both metric as some sign ups will go on to activate (probably 10% additional), and users who churned will get a nudge to try new features, or at the very least give it a second try. 

## 2: Allow for the Text-to-agent product in line to generate the code, including tool calls, logic, transfers, and metrics.

As of today, Text-to-agent can be used to create a single-prompt voice agent. My suspicion is that without tool-calling (and secondarily logic, transfers, metrics), these agents are not useful to the majority of business use-cases, which I assume are a majority of the use cases that would use Line.

I would add functionality to have the Text-to-agent generate tool-calls, logic, transfer/prompt updates, and even a few relevant metrics. This helps builders realize their vision for what they're building more quickly, and instead of having an AI middle-man via Cursor helping them architect their Agent, Cartesia's AI, which would also have context on how best to make the agent Cartesia ready, would create the agent. I still think it can live in code, but connecting Git could become optional if the agent can be managed through the UI playground.

The primary metric this would drive is activations on the Line product. More agents could be built faster, and ideally if we do a good job, deployed to production. 

